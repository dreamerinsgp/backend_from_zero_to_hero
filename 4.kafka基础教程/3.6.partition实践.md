# Partition 实践实验

> 目标：通过实际实验加深对 Partition 中每个文件作用的理解

---

## 实验前置准备

### 1. 环境要求

- ✅ Kafka 已安装并运行
- ✅ 有测试 Topic（如 `test-partition-exp`）
- ✅ 已发送一些测试消息

### 2. 准备测试数据

```bash
# 1. 创建测试 Topic（3 个 Partition）
cd ~/kafka/kafka_2.13-3.6.1
bin/kafka-topics.sh --create \
  --topic test-partition-exp \
  --bootstrap-server localhost:9092 \
  --partitions 3 \
  --replication-factor 1

# 2. 发送测试消息（带 Key，推荐方式）
for i in {1..10}; do
  echo "key-$i:Message $i at $(date +%s)000" | \
  bin/kafka-console-producer.sh \
    --topic test-partition-exp \
    --bootstrap-server localhost:9092 \
    --property "parse.key=true" \
    --property "key.separator=:"
done

# 或者发送不带 Key 的消息（使用轮询分区）
for i in {1..10}; do
  echo "Message $i" | \
  bin/kafka-console-producer.sh \
    --topic test-partition-exp \
    --bootstrap-server localhost:9092
done

# 3. 验证消息已发送
bin/kafka-console-consumer.sh \
  --topic test-partition-exp \
  --bootstrap-server localhost:9092 \
  --from-beginning \
  --property print.key=true \
  --property print.timestamp=true \
  --max-messages 10
```

### 3. 找到 Partition 文件位置

```bash
# 查看数据目录
LOG_DIR=$(grep "^log.dirs" ~/kafka/kafka_2.13-3.6.1/config/kraft/server.properties | cut -d'=' -f2)
echo "数据目录: $LOG_DIR"

# 查看测试 Topic 的 Partition
ls -d $LOG_DIR/test-partition-exp-*

# 选择一个 Partition（如 Partition 0）
PARTITION_DIR="$LOG_DIR/test-partition-exp-0"
echo "Partition 目录: $PARTITION_DIR"
ls -lh $PARTITION_DIR/
```

---

## 实验 1：如何查看 Partition Log 中的消息

### 实验目标

- ✅ 理解 `.log` 文件的二进制格式
- ✅ 学会使用 Kafka 工具查看 log 文件内容
- ✅ 理解消息在 log 文件中的存储方式

---

### 步骤 1：查看 Log 文件基本信息

```bash
# 1. 查看 log 文件大小
ls -lh $PARTITION_DIR/*.log

# 2. 查看文件是二进制格式（不能直接用 cat 查看）
file $PARTITION_DIR/00000000000000000000.log
# 输出: gzip compressed data 或 data

# 3. 尝试直接查看（会看到乱码）
hexdump -C $PARTITION_DIR/00000000000000000000.log | head -20
```

**观察结果**：
- Log 文件是二进制格式，不能直接用文本工具查看
- 需要使用 Kafka 提供的工具来解析

---

### 步骤 2：使用 Kafka Dump Log Segments 工具

```bash
cd ~/kafka/kafka_2.13-3.6.1

# 使用 kafka-dump-log.sh 查看 log 文件内容
bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/00000000000000000000.log \
  --print-data-log
```

**输出示例**：
```
Dumping /tmp/kraft-combined-logs/test-partition-exp-0/00000000000000000000.log
Starting offset: 0
baseOffset: 0 lastOffset: 0 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: 0 CreateTime: 1704067200000 size: 72 magic: 2 compresscodec: NONE crc: 1234567890 isvalid: true
| offset: 0 CreateTime: 1704067200000 keysize: 4 keysize: 4 key: key-1 payload: Message 1
| offset: 1 CreateTime: 1704067300000 keysize: 4 keysize: 4 key: key-2 payload: Message 2
| offset: 2 CreateTime: 1704067400000 keysize: 4 keysize: 4 key: key-3 payload: Message 3
...
```

**关键信息解读**：
- `offset`: 消息的偏移量（0, 1, 2, ...）
- `CreateTime`: 消息创建时间戳（毫秒）
- `key`: 消息的 Key
- `payload`: 消息内容
- `position`: 消息在文件中的位置（字节）

---

### 步骤 3：理解 Log 文件格式

```bash
# 查看更详细的信息，包括文件位置
# 注意：--print-data-log 选项已经足够显示消息内容
# 如果需要自定义解码器，可以使用 --value-decoder-class 指定实现了 Decoder 接口的类
bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/00000000000000000000.log \
  --print-data-log
```

**Log 文件内部结构**：

```
[消息批次头]
  - baseOffset: 批次起始 Offset
  - lastOffset: 批次结束 Offset
  - count: 批次中消息数量
  - CreateTime: 批次创建时间
  - position: 批次在文件中的位置

[消息 1]
  - offset: 0
  - key: "key-1"
  - payload: "Message 1"
  - CreateTime: 1704067200000

[消息 2]
  - offset: 1
  - key: "key-2"
  - payload: "Message 2"
  - CreateTime: 1704067300000
...
```

---

### 步骤 4：对比直接读取 vs Kafka 工具读取

```bash
# 方法 1：直接读取（不可读）
cat $PARTITION_DIR/00000000000000000000.log
# 输出：乱码或二进制数据

# 方法 2：使用 Kafka 工具（可读）
bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/00000000000000000000.log \
  --print-data-log

# 方法 3：使用 Consumer 读取（推荐的生产方式）
bin/kafka-console-consumer.sh \
  --topic test-partition-exp \
  --bootstrap-server localhost:9092 \
  --partition 0 \
  --offset 0 \
  --max-messages 10
```

**结论**：
- ✅ Log 文件是二进制格式，必须使用 Kafka 工具或 Consumer API 读取
- ✅ `kafka-dump-log.sh` 是查看 log 文件内容的工具
- ✅ 生产环境应该使用 Consumer API 而不是直接读取文件

---

### 实验 1 总结

**关键理解**：
1. ✅ **Log 文件格式**：二进制格式，包含消息的完整信息
2. ✅ **查看工具**：使用 `kafka-dump-log.sh` 查看 log 文件
3. ✅ **消息结构**：每条消息包含 offset、key、payload、timestamp 等信息
4. ✅ **追加写入**：消息按顺序追加到文件末尾

---

## 实验 2：如何通过 Index 加速消息查询

### 实验目标

- ✅ 理解 Index 文件的作用
- ✅ 对比有无 Index 的查询性能差异
- ✅ 理解稀疏索引的工作原理

---

### 步骤 1：查看 Index 文件

```bash
# 1. 查看 index 文件大小
ls -lh $PARTITION_DIR/*.index

# 2. 验证 index 文件完整性（只检查，不打印内容）
bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/00000000000000000000.index \
  --index-sanity-check

# 3. 查看 index 文件内容（显示 offset 和 position 映射）
bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/00000000000000000000.index
```

**输出示例**（验证命令）：
```
Dumping /tmp/kraft-combined-logs/test-partition-exp-0/00000000000000000000.index
/tmp/kraft-combined-logs/test-partition-exp-0/00000000000000000000.index passed sanity check.
```

**输出示例**（查看内容命令）：
```
Dumping /tmp/kraft-combined-logs/test-partition-exp-0/00000000000000000000.index
offset: 0 position: 0
offset: 10 position: 1024
offset: 20 position: 2048
offset: 30 position: 3072
...
```

**Index 文件格式**：
- `offset`: 消息的偏移量
- `position`: 消息在 log 文件中的位置（字节）

---

### 步骤 2：理解稀疏索引

```bash
# 查看 index 文件中的索引条目数量（需要去掉 --index-sanity-check 才能看到内容）
bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/00000000000000000000.index | \
  grep "offset:" | wc -l

# 对比 log 文件中的消息数量
bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/00000000000000000000.log \
  --print-data-log | grep "offset:" | wc -l
```

**观察结果**：
- Index 文件中的条目数量 << Log 文件中的消息数量
- 说明 Index 是**稀疏索引**，不是每条消息都有索引

---

### 步骤 3：模拟查询过程（无 Index）

```bash
# 模拟：查找 Offset = 50 的消息（无 Index 的情况）
# 需要从头扫描整个 log 文件

time bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/00000000000000000000.log \
  --print-data-log | grep "offset: 50"
```

**性能分析**：
- 需要读取整个 log 文件
- 时间复杂度：O(n)，n 为消息总数
- 如果 log 文件很大（如 1GB），查询会很慢

---

### 步骤 4：模拟查询过程（有 Index）

```bash
# 模拟：查找 Offset = 50 的消息（有 Index 的情况）
# 1. 先在 Index 中找到最接近的索引点（去掉 --index-sanity-check 才能看到内容）
bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/00000000000000000000.index | \
  awk '/^offset:/ {offset=$2; if (offset <= 50) print offset, $4} END {print offset, $4}' | tail -1

# 输出示例：offset: 40 position: 4096
# 说明：Offset 50 的消息在 position 4096 之后

# 2. 从该 position 开始扫描 log 文件（只需要扫描一小部分）
# 使用 dd 命令从指定位置开始读取
dd if=$PARTITION_DIR/00000000000000000000.log \
   bs=1 skip=4096 \
   count=10240 2>/dev/null | \
   hexdump -C | head -20
```

**性能分析**：
- 只需要读取 Index 文件（很小）和 log 文件的一小部分
- 时间复杂度：O(log n) + O(m)，m 为稀疏索引间隔
- 查询速度大幅提升

---

### 步骤 5：性能对比实验

创建一个简单的性能测试脚本：

```bash
#!/bin/bash
# 文件名：test-index-performance.sh

PARTITION_DIR="/tmp/kraft-combined-logs/test-partition-exp-0"
TARGET_OFFSET=100

echo "=== 性能对比实验 ==="
echo "查找 Offset = $TARGET_OFFSET 的消息"
echo ""

# 方法 1：无 Index（全扫描）
echo "方法 1：无 Index（全扫描）"
time bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/00000000000000000000.log \
  --print-data-log | grep "offset: $TARGET_OFFSET" > /dev/null
echo ""

# 方法 2：有 Index（快速定位）
echo "方法 2：有 Index（快速定位）"
# 先查找 Index（去掉 --index-sanity-check 才能看到内容）
NEAREST_OFFSET=$(bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/00000000000000000000.index 2>/dev/null | \
  awk -v target=$TARGET_OFFSET '/^offset:/ {offset=$2; if (offset <= target) print offset}' | tail -1)

if [ -n "$NEAREST_OFFSET" ]; then
  echo "找到最近的索引点: Offset $NEAREST_OFFSET"
  # 从该位置开始查找
  time bin/kafka-dump-log.sh \
    --files $PARTITION_DIR/00000000000000000000.log \
    --print-data-log | \
    awk -v target=$TARGET_OFFSET '/offset: / {if ($2 == target) print; if ($2 > target) exit}' > /dev/null
else
  echo "未找到索引点，需要全扫描"
fi
```

**运行测试**：
```bash
chmod +x test-index-performance.sh
./test-index-performance.sh
```

---

### 步骤 6：理解 Index 的工作原理

**Index 查找流程**：

```
1. Consumer 请求 Offset = 50 的消息

2. 查找 Index 文件：
   - 找到最接近且 <= 50 的索引点
   - 例如：Offset 40 → Position 4096

3. 从 Position 4096 开始扫描 log 文件：
   - 顺序读取消息
   - 直到找到 Offset = 50 的消息

4. 返回消息内容
```

**为什么 Index 能加速查询？**

- ✅ **减少扫描范围**：从整个文件缩小到索引点之后的一小段
- ✅ **快速定位**：Index 文件很小，可以快速加载到内存
- ✅ **稀疏索引**：不需要为每条消息建立索引，节省空间

---

### 实验 2 总结

**关键理解**：
1. ✅ **Index 作用**：提供 Offset 到文件位置的快速映射
2. ✅ **稀疏索引**：不是每条消息都有索引，节省存储空间
3. ✅ **性能提升**：使用 Index 可以大幅提升查询速度
4. ✅ **查找流程**：Index → 定位位置 → 扫描小段 → 找到消息

---

## 实验 3：如何通过 TimeIndex 进行时间范围查询

### 实验目标

- ✅ 理解 TimeIndex 文件的作用
- ✅ 学会按时间范围查询消息
- ✅ 理解时间戳到 Offset 的映射关系

---

### 步骤 1：查看 TimeIndex 文件

```bash
# 1. 查看 timeindex 文件大小
ls -lh $PARTITION_DIR/*.timeindex

# 2. 验证 timeindex 文件完整性（只检查，不打印内容）
bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/00000000000000000000.timeindex \
  --index-sanity-check

# 3. 查看 timeindex 文件内容（显示 timestamp 和 offset 映射）
bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/00000000000000000000.timeindex
```

**输出示例**（验证命令）：
```
Dumping /tmp/kraft-combined-logs/test-partition-exp-0/00000000000000000000.timeindex
/tmp/kraft-combined-logs/test-partition-exp-0/00000000000000000000.timeindex passed sanity check.
```

**输出示例**（查看内容命令）：
```
Dumping /tmp/kraft-combined-logs/test-partition-exp-0/00000000000000000000.timeindex
timestamp: 1704067200000 offset: 0
timestamp: 1704067300000 offset: 10
timestamp: 1704067400000 offset: 20
timestamp: 1704067500000 offset: 30
...
```

**TimeIndex 文件格式**：
- `timestamp`: 消息的时间戳（毫秒）
- `offset`: 消息的偏移量

**重要说明 - TimeIndex 是稀疏索引**：
- ⚠️ **只有当时间戳递增时才会添加条目**：TimeIndex 只在 `timestamp > 上一个timestamp` 时添加新条目
- ⚠️ **如果所有消息时间戳相同**：TimeIndex 可能只有一个条目（初始条目）
- ⚠️ **如果消息格式是 v0（无时间戳）**：TimeIndex 可能是空的或只有初始条目
- ⚠️ **Timestamp mismatch 警告**：如果看到 "Found timestamp mismatch" 警告，说明 timeindex 中的时间戳和 log 文件中的实际时间戳不匹配，可能需要重建索引

**为什么 TimeIndex 条目可能很少？**
```bash
# 查看 log 文件中的实际时间戳
bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/00000000000000000000.log \
  --print-data-log | grep "CreateTime"

# 如果所有消息的 CreateTime 都相同，TimeIndex 只会有一个条目
# 只有当时间戳递增时，TimeIndex 才会添加新条目
```

---

### 步骤 2：理解时间戳格式

```bash
# 查看消息的时间戳
bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/00000000000000000000.log \
  --print-data-log | grep "CreateTime"

# 时间戳转换示例
TIMESTAMP=1704067200000
echo "时间戳: $TIMESTAMP"
echo "转换为日期: $(date -d @$(echo "$TIMESTAMP / 1000" | bc))"
```

**时间戳说明**：
- Kafka 使用毫秒级时间戳（13 位数字）
- Unix 时间戳是秒级（10 位数字）
- 转换：`Kafka 时间戳 / 1000 = Unix 时间戳`

**为什么 TimeIndex 可能只有很少条目？**

根据 Kafka 源码，TimeIndex 是**稀疏索引**，有以下特点：

1. **只有当时间戳递增时才添加条目**：
   ```java
   // TimeIndex.java 中的逻辑
   if (timestamp > lastEntry.timestamp()) {
       // 只有当新时间戳大于上一个时间戳时，才添加索引条目
   }
   ```

2. **如果所有消息时间戳相同**：
   - TimeIndex 可能只有一个初始条目（如 `timestamp: 0 offset: 0`）
   - 即使有 1000 条消息，如果时间戳都相同，TimeIndex 也只有一个条目

3. **Timestamp mismatch 警告的含义**：
   ```
   Found timestamp mismatch in :/tmp/.../00000000000000000000.timeindex
     Index timestamp: 0, log timestamp: 176995997011
   ```
   - 这表示 timeindex 中记录的时间戳（0）和 log 文件中的实际时间戳（176995997011）不匹配
   - 可能原因：
     - TimeIndex 没有正确更新（消息时间戳没有递增）
     - TimeIndex 文件损坏或不完整
     - 消息格式问题（v0 格式没有时间戳）

**如何诊断**：
```bash
# 1. 查看 log 文件中的实际时间戳分布
bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/00000000000000000000.log \
  --print-data-log | grep "CreateTime" | head -20

# 2. 检查时间戳是否都相同
bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/00000000000000000000.log \
  --print-data-log | grep "CreateTime" | awk '{print $2}' | sort -u | wc -l

# 如果输出是 1，说明所有消息的时间戳都相同，TimeIndex 只会有一个条目
```

---

### 步骤 3：按时间范围查询消息（无 TimeIndex）

```bash
# 方法 1：扫描所有消息，过滤时间戳（慢）
START_TIME=1704067200000  # 2024-01-01 00:00:00
END_TIME=1704067300000    # 2024-01-01 00:01:40

bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/00000000000000000000.log \
  --print-data-log | \
  awk -v start=$START_TIME -v end=$END_TIME \
  '/CreateTime: / {ts=$2; if (ts >= start && ts <= end) print}'
```

**性能问题**：
- 需要扫描所有消息
- 时间复杂度：O(n)
- 如果消息很多，查询会很慢

---

### 步骤 4：按时间范围查询消息（有 TimeIndex）

```bash
# 方法 2：使用 TimeIndex 快速定位（快）

# 1. 在 TimeIndex 中找到时间范围对应的 Offset 范围
START_TIME=1704067200000
END_TIME=1704067300000

# 查找起始 Offset（>= START_TIME 的第一个）（去掉 --index-sanity-check 才能看到内容）
START_OFFSET=$(bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/00000000000000000000.timeindex 2>/dev/null | \
  awk -v start=$START_TIME '/^timestamp:/ {ts=$2; offset=$4; if (ts >= start) {print offset; exit}}')

# 查找结束 Offset（<= END_TIME 的最后一个）
END_OFFSET=$(bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/00000000000000000000.timeindex 2>/dev/null | \
  awk -v end=$END_TIME '/^timestamp:/ {ts=$2; offset=$4; if (ts <= end) offset_val=offset} END {print offset_val}')

echo "时间范围: $START_TIME - $END_TIME"
echo "Offset 范围: $START_OFFSET - $END_OFFSET"

# 2. 使用 Offset 范围查询消息（使用 Consumer API）
bin/kafka-console-consumer.sh \
  --topic test-partition-exp \
  --bootstrap-server localhost:9092 \
  --partition 0 \
  --offset $START_OFFSET \
  --max-messages $((END_OFFSET - START_OFFSET + 1)) \
  --property print.timestamp=true
```

**性能优势**：
- 只需要查询 TimeIndex（很小）
- 然后根据 Offset 范围查询，不需要扫描所有消息
- 时间复杂度：O(log n) + O(m)，m 为时间范围内的消息数

---

### 步骤 5：使用 Kafka Consumer API 按时间查询

```bash
# Kafka 提供了按时间查询的 Consumer API
# 但命令行工具支持有限，通常需要在代码中使用

# 方法：使用 Consumer API 的 seek 功能
# 这里演示使用 kafka-console-consumer 的替代方案

# 1. 先找到时间对应的 Offset（使用 TimeIndex）
TARGET_TIME=1704067200000

# 2. 使用 Consumer 从该 Offset 开始消费
bin/kafka-console-consumer.sh \
  --topic test-partition-exp \
  --bootstrap-server localhost:9092 \
  --partition 0 \
  --property print.timestamp=true \
  --property print.key=true
```

**实际应用**：
在代码中使用 Consumer API：
```go
// Go 示例（使用 kafka-go）
reader := kafka.NewReader(kafka.ReaderConfig{
    Brokers:  []string{"localhost:9092"},
    Topic:    "test-partition-exp",
    Partition: 0,
})

// 查找指定时间之后的第一个 Offset
// Kafka 内部会使用 TimeIndex 来加速查找
```

---

### 步骤 6：创建时间查询脚本

```bash
#!/bin/bash
# 文件名：query-by-time.sh
# 功能：根据时间范围查询消息

PARTITION_DIR="/tmp/kraft-combined-logs/test-partition-exp-0"
TOPIC="test-partition-exp"
BROKER="localhost:9092"
PARTITION=0

# 参数：开始时间戳（毫秒）和结束时间戳（毫秒）
START_TIME=${1:-1704067200000}
END_TIME=${2:-1704067300000}

echo "=== 按时间范围查询消息 ==="
echo "时间范围: $START_TIME - $END_TIME"
echo ""

# 转换时间戳为可读格式
START_DATE=$(date -d @$(echo "$START_TIME / 1000" | bc) 2>/dev/null || echo "N/A")
END_DATE=$(date -d @$(echo "$END_TIME / 1000" | bc) 2>/dev/null || echo "N/A")
echo "开始时间: $START_DATE"
echo "结束时间: $END_DATE"
echo ""

# 使用 TimeIndex 查找 Offset 范围
cd ~/kafka/kafka_2.13-3.6.1

START_OFFSET=$(bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/00000000000000000000.timeindex 2>/dev/null | \
  awk -v start=$START_TIME '/^timestamp:/ {ts=$2; offset=$4; if (ts >= start) {print offset; exit}}')

END_OFFSET=$(bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/00000000000000000000.timeindex 2>/dev/null | \
  awk -v end=$END_TIME '/^timestamp:/ {ts=$2; offset=$4; if (ts <= end) offset_val=offset} END {print offset_val}')

if [ -z "$START_OFFSET" ] || [ -z "$END_OFFSET" ]; then
  echo "未找到时间范围内的消息"
  exit 1
fi

echo "Offset 范围: $START_OFFSET - $END_OFFSET"
echo ""

# 查询消息
echo "=== 查询结果 ==="
bin/kafka-console-consumer.sh \
  --topic $TOPIC \
  --bootstrap-server $BROKER \
  --partition $PARTITION \
  --offset $START_OFFSET \
  --max-messages $((END_OFFSET - START_OFFSET + 1)) \
  --property print.timestamp=true \
  --property print.key=true \
  2>/dev/null
```

**使用方法**：
```bash
chmod +x query-by-time.sh

# 查询指定时间范围的消息
./query-by-time.sh 1704067200000 1704067300000
```

---

### 步骤 7：对比有无 TimeIndex 的查询性能

```bash
#!/bin/bash
# 性能对比脚本

PARTITION_DIR="/tmp/kraft-combined-logs/test-partition-exp-0"
START_TIME=1704067200000
END_TIME=1704067300000

echo "=== TimeIndex 性能对比 ==="
echo ""

# 方法 1：无 TimeIndex（全扫描）
echo "方法 1：无 TimeIndex（全扫描所有消息）"
time bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/00000000000000000000.log \
  --print-data-log | \
  awk -v start=$START_TIME -v end=$END_TIME \
  '/CreateTime: / {ts=$2; if (ts >= start && ts <= end) count++} END {print "找到消息数:", count+0}'
echo ""

# 方法 2：有 TimeIndex（快速定位）
echo "方法 2：有 TimeIndex（使用索引快速定位）"
time {
  START_OFFSET=$(bin/kafka-dump-log.sh \
    --files $PARTITION_DIR/00000000000000000000.timeindex 2>/dev/null | \
    awk -v start=$START_TIME '/^timestamp:/ {ts=$2; offset=$4; if (ts >= start) {print offset; exit}}')
  
  END_OFFSET=$(bin/kafka-dump-log.sh \
    --files $PARTITION_DIR/00000000000000000000.timeindex 2>/dev/null | \
    awk -v end=$END_TIME '/^timestamp:/ {ts=$2; offset=$4; if (ts <= end) offset_val=offset} END {print offset_val}')
  
  if [ -n "$START_OFFSET" ] && [ -n "$END_OFFSET" ]; then
    COUNT=$((END_OFFSET - START_OFFSET + 1))
    echo "找到消息数: $COUNT (Offset: $START_OFFSET - $END_OFFSET)"
  fi
}
```

---

### 实验 3 总结

**关键理解**：
1. ✅ **TimeIndex 作用**：提供时间戳到 Offset 的映射
2. ✅ **时间查询**：支持按时间范围快速查询消息
3. ✅ **性能提升**：使用 TimeIndex 可以快速定位时间范围对应的 Offset 范围
4. ✅ **实际应用**：日志审计、时间回溯、按时间消费等场景

---

## 综合实验：完整的分区文件理解

### 实验目标

通过一个完整的场景，理解三个文件如何协同工作。

---

### 场景：查找特定时间点的消息内容

**需求**：查找 2024-01-01 00:00:00 之后的第一条消息

**步骤**：

```bash
#!/bin/bash
# 完整查询流程演示

PARTITION_DIR="/tmp/kraft-combined-logs/test-partition-exp-0"
TARGET_TIME=1704067200000  # 2024-01-01 00:00:00

echo "=== 完整查询流程 ==="
echo "目标时间: $(date -d @$(echo "$TARGET_TIME / 1000" | bc))"
echo ""

cd ~/kafka/kafka_2.13-3.6.1

# 步骤 1：使用 TimeIndex 找到时间对应的 Offset
echo "步骤 1：使用 TimeIndex 查找 Offset"
TARGET_OFFSET=$(bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/00000000000000000000.timeindex 2>/dev/null | \
  awk -v target=$TARGET_TIME '/^timestamp:/ {ts=$2; offset=$4; if (ts >= target) {print offset; exit}}')

if [ -z "$TARGET_OFFSET" ]; then
  echo "未找到目标时间之后的消息"
  exit 1
fi

echo "找到 Offset: $TARGET_OFFSET"
echo ""

# 步骤 2：使用 Index 找到 Offset 对应的文件位置
echo "步骤 2：使用 Index 查找文件位置"
FILE_POSITION=$(bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/00000000000000000000.index 2>/dev/null | \
  awk -v target=$TARGET_OFFSET '/^offset:/ {offset=$2; pos=$4; if (offset <= target) position=pos} END {print position}')

echo "文件位置: $FILE_POSITION 字节"
echo ""

# 步骤 3：从文件位置读取消息内容
echo "步骤 3：从 Log 文件读取消息内容"
bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/00000000000000000000.log \
  --print-data-log | \
  awk -v target=$TARGET_OFFSET '/offset: / {if ($2 == target) {found=1} if (found) print; if ($2 > target && found) exit}'
```

**流程总结**：

```
1. TimeIndex: 时间戳 → Offset
   TARGET_TIME (1704067200000) → Offset (10)

2. Index: Offset → 文件位置
   Offset (10) → Position (1024)

3. Log: 文件位置 → 消息内容
   Position (1024) → Message Content
```

---

## 实验总结

### 三个文件的作用和关系

| 文件 | 作用 | 查询路径 | 性能 |
|------|------|---------|------|
| **`.log`** | 存储消息数据 | 直接读取 | 慢（需要全扫描） |
| **`.index`** | Offset → 位置映射 | Offset → Position → Log | 快（稀疏索引） |
| **`.timeindex`** | 时间戳 → Offset 映射 | Time → Offset → Index → Position → Log | 快（时间索引） |

### 查询优化流程

```
按时间查询：
TimeIndex (Time → Offset) → Index (Offset → Position) → Log (Position → Content)

按 Offset 查询：
Index (Offset → Position) → Log (Position → Content)

直接查询（不推荐）：
Log (全扫描) → Content
```

### 关键要点

1. ✅ **Log 文件**：存储实际消息，是数据源
2. ✅ **Index 文件**：加速 Offset 查询，避免全扫描
3. ✅ **TimeIndex 文件**：支持按时间查询，先转 Offset 再查
4. ✅ **协同工作**：三个文件配合，实现高效的消息查询

---

## 扩展实验

### 实验 4：观察 Segment 滚动

```bash
# 1. 发送大量消息，触发 Segment 滚动
for i in {1..1000}; do
  echo "Message $i" | \
  bin/kafka-console-producer.sh \
    --topic test-partition-exp \
    --bootstrap-server localhost:9092
done

# 2. 观察是否创建了新的 Segment 文件
ls -lh $PARTITION_DIR/*.log

# 3. 查看新 Segment 的起始 Offset
bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/*.log \
  --print-data-log | head -1
```

### 实验 5：理解索引的稀疏性

```bash
# 统计索引密度（去掉 --index-sanity-check 才能看到内容）
INDEX_COUNT=$(bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/00000000000000000000.index 2>/dev/null | \
  grep "^offset:" | wc -l)

MESSAGE_COUNT=$(bin/kafka-dump-log.sh \
  --files $PARTITION_DIR/00000000000000000000.log \
  --print-data-log | grep "offset:" | wc -l)

echo "索引条目数: $INDEX_COUNT"
echo "消息总数: $MESSAGE_COUNT"
echo "索引密度: $(echo "scale=2; $INDEX_COUNT * 100 / $MESSAGE_COUNT" | bc)%"
```

---

**相关文档**：
- Partition 文件说明：`3.5.每个parition包含哪些内容及含义.md`
- Partition 存储位置：`3.1.partition存储位置.md`
- Kafka 工具使用：Kafka 官方文档
